{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733669ce",
   "metadata": {},
   "source": [
    "# Classification d'images avec EfficientNetB5\n",
    "Ce projet consiste à entraîner un modèle de deep learning pour classer des images en trois catégories (par exemple : tumeur cérébrale, sain, etc.) à l'aide d'**EfficientNetB5** et de **Keras/TensorFlow**.\n",
    "\n",
    "Le jeu de données est supposé être structuré dans un dossier `data/` avec un sous-dossier par classe contenant les images.\n",
    "\n",
    "**Phases couvertes :**\n",
    "- Nettoyage des doublons\n",
    "- Partition en train/val/test\n",
    "- Prétraitement des données\n",
    "- Entraînement avec callbacks\n",
    "- Évaluation et visualisation des performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860b190",
   "metadata": {},
   "source": [
    "## 1. Imports et dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f58067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB5\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf57378",
   "metadata": {},
   "source": [
    "## 2. Détection et suppression de doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609258fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def file_hash(path, algo='md5', block_size=65536):\n",
    "    h = hashlib.new(algo)\n",
    "    with open(path, 'rb') as f:\n",
    "        for block in iter(lambda: f.read(block_size), b''):\n",
    "            h.update(block)\n",
    "    return h.hexdigest()\n",
    "\n",
    "root_dir = 'data'\n",
    "seen = {}\n",
    "to_remove = []\n",
    "\n",
    "for cls in os.listdir(root_dir):\n",
    "    cls_path = os.path.join(root_dir, cls)\n",
    "    if not os.path.isdir(cls_path):\n",
    "        continue\n",
    "    for fname in os.listdir(cls_path):\n",
    "        path = os.path.join(cls_path, fname)\n",
    "        if not os.path.isfile(path):\n",
    "            continue\n",
    "        h = file_hash(path)\n",
    "        if h in seen:\n",
    "            to_remove.append(path)\n",
    "        else:\n",
    "            seen[h] = path\n",
    "\n",
    "for dup in to_remove:\n",
    "    os.remove(dup)\n",
    "    print(f\"Supprimé : {dup}\")\n",
    "\n",
    "print(f\"Doublons supprimés : {len(to_remove)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8968c48",
   "metadata": {},
   "source": [
    "## 3. Répartition des données en train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028332c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_dir = 'data'\n",
    "target_dir = 'data_ml_efficient_net'\n",
    "classes = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
    "ratios = {'train': 0.7, 'val': 0.15, 'test': 0.15}\n",
    "\n",
    "# Nettoyage du dossier cible\n",
    "if os.path.exists(target_dir):\n",
    "    shutil.rmtree(target_dir)\n",
    "for split in ratios:\n",
    "    for cls in classes:\n",
    "        os.makedirs(os.path.join(target_dir, split, cls), exist_ok=True)\n",
    "\n",
    "# Répartition\n",
    "random.seed(42)\n",
    "for cls in classes:\n",
    "    files = os.listdir(os.path.join(source_dir, cls))\n",
    "    random.shuffle(files)\n",
    "    n = len(files)\n",
    "    train, val = int(n * ratios['train']), int(n * ratios['val'])\n",
    "    splits = {\n",
    "        'train': files[:train],\n",
    "        'val': files[train:train + val],\n",
    "        'test': files[train + val:]\n",
    "    }\n",
    "    for split, file_list in splits.items():\n",
    "        for f in file_list:\n",
    "            shutil.copy(os.path.join(source_dir, cls, f), os.path.join(target_dir, split, cls, f))\n",
    "\n",
    "print('Données réparties dans train/val/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ffed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "split_hashes = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_hashes[split] = set()\n",
    "    for path in glob(f\"{target_dir}/{split}/**/*.*\", recursive=True):\n",
    "        split_hashes[split].add(file_hash(path))\n",
    "\n",
    "# Chercher les intersections\n",
    "for a in split_hashes:\n",
    "    for b in split_hashes:\n",
    "        if a < b:\n",
    "            inter = split_hashes[a] & split_hashes[b]\n",
    "            if inter:\n",
    "                print(f\"Doublons entre {a} et {b} : {len(inter)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, total = count_images(target_dir)\n",
    "\n",
    "print(f\"TOTAL images dans '{target_dir}': {total}\\n\")\n",
    "print(\"Détail par sous-dossier :\")\n",
    "for subdir, n in sorted(counts.items()):\n",
    "    print(f\"  {subdir:30s} : {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c8031",
   "metadata": {},
   "source": [
    "## 4. Chargement des images avec `ImageDataGenerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "train_dir = os.path.join(target_dir, 'train')\n",
    "val_dir = os.path.join(target_dir, 'val')\n",
    "test_dir = os.path.join(target_dir, 'test')\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1./255, rotation_range=15, zoom_range=0.2, horizontal_flip=True).flow_from_directory(\n",
    "    train_dir, target_size=image_size, batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "val_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    val_dir, target_size=image_size, batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    test_dir, target_size=image_size, batch_size=batch_size, class_mode='categorical', shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d2e90",
   "metadata": {},
   "source": [
    "## 5. Construction du modèle EfficientNetB5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97837931",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = EfficientNetB5(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers[:95]:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(len(classes), activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba605c",
   "metadata": {},
   "source": [
    "## 6. Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=50, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2fdc0",
   "metadata": {},
   "source": [
    "## 7. Évaluation et visualisation des performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3813ab",
   "metadata": {},
   "source": [
    "## Evolution des métrics durant le train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraire les métriques depuis l'objet history\n",
    "val_accuracy = np.asarray(history.history['val_accuracy'])\n",
    "train_accuracy = np.asarray(history.history['accuracy'])\n",
    "val_loss = np.asarray(history.history['val_loss'])\n",
    "train_loss = np.asarray(history.history['loss'])\n",
    "\n",
    "# Nombre d'époques\n",
    "epochs = np.arange(1, len(train_accuracy) + 1)\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_accuracy, label='Exactitude (entraînement)', marker='o', color='green')\n",
    "plt.plot(epochs, val_accuracy, label='Exactitude (validation)', marker='o', color='blue')\n",
    "plt.plot(epochs, train_loss, label='Perte (entraînement)', marker='s', color='red')\n",
    "plt.plot(epochs, val_loss, label='Perte (validation)', marker='s', color='orange')\n",
    "\n",
    "plt.title(\"Évolution de l'exactitude et de la perte pendant l'entraînement\")\n",
    "plt.xlabel(\"Époques\")\n",
    "plt.ylabel(\"Valeurs\")\n",
    "plt.legend(loc='center right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ed601",
   "metadata": {},
   "source": [
    "## Evaluation de la performance du modèle sur les données de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss, acc = model.evaluate(test_gen)\n",
    "print(f\"Test Accuracy: {acc:.4f} | Test Loss: {loss:.4f}\")\n",
    "\n",
    "y_pred_probs = model.predict(test_gen)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = test_gen.classes\n",
    "class_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.xlabel(\"Prédictions\")\n",
    "plt.ylabel(\"Vérités\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
